{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-29T06:58:46.645599Z",
     "start_time": "2025-03-29T06:58:23.526331Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    T5ForConditionalGeneration, \n",
    "    T5Tokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "MODEL_TYPE = \"bert\"  # or \"t5\"\n",
    "BERT_MODEL_NAME = \"dmis-lab/biobert-v1.1\"  # or \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "T5_MODEL_NAME = \"razent/SciFive-base-Pubmed_PMC\"  # or \"t5-small\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "OUTPUT_DIR = \"./space_biology_model\"\n",
    "\n",
    "# 1. Dataset Collection and Preprocessing\n",
    "\n",
    "def create_dataset_catalog():\n",
    "    \"\"\"Create a catalog of available biomedical and space biology datasets.\"\"\"\n",
    "    \n",
    "    biomedical_datasets = [\n",
    "        {\n",
    "            \"name\": \"PubMed Abstracts (Space Biology)\",\n",
    "            \"source\": \"PubMed\",\n",
    "            \"data_type\": \"text\",\n",
    "            \"size\": \"~10,000 abstracts\",\n",
    "            \"url\": \"https://pubmed.ncbi.nlm.nih.gov/\",\n",
    "            \"description\": \"Scientific abstracts related to space biology research\",\n",
    "            \"domains\": [\"microgravity\", \"radiation\", \"space adaptation\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"UniProt Space Biology Entries\",\n",
    "            \"source\": \"UniProt\",\n",
    "            \"data_type\": \"protein annotations\",\n",
    "            \"size\": \"~1,000 entries\",\n",
    "            \"url\": \"https://www.uniprot.org/\",\n",
    "            \"description\": \"Protein annotations for organisms studied in space\",\n",
    "            \"domains\": [\"protein function\", \"structural biology\"]\n",
    "        }\n",
    "        # Add more datasets as needed\n",
    "    ]\n",
    "    \n",
    "    space_biology_datasets = [\n",
    "        {\n",
    "            \"name\": \"NASA GeneLab\",\n",
    "            \"source\": \"NASA\",\n",
    "            \"data_type\": \"omics data + metadata\",\n",
    "            \"size\": \"500+ datasets\",\n",
    "            \"url\": \"https://genelab.nasa.gov/\",\n",
    "            \"description\": \"Omics data from space biology experiments\",\n",
    "            \"domains\": [\"transcriptomics\", \"proteomics\", \"genomics\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"OSDR WGS Dataset 466\",\n",
    "            \"source\": \"NASA OSDR\",\n",
    "            \"data_type\": \"genomic + metadata\",\n",
    "            \"size\": \"1 dataset\",\n",
    "            \"url\": \"https://osdr.nasa.gov/bio/repo/data/studies/OSD-466\",\n",
    "            \"description\": \"Whole Genome Sequencing data with descriptions\",\n",
    "            \"domains\": [\"genomics\", \"microbiology\"]\n",
    "        }\n",
    "        # Add more datasets as needed\n",
    "    ]\n",
    "    \n",
    "    # Save to CSV files\n",
    "    pd.DataFrame(biomedical_datasets).to_csv(\"biomedical_datasets.csv\", index=False)\n",
    "    pd.DataFrame(space_biology_datasets).to_csv(\"space_biology_datasets.csv\", index=False)\n",
    "    \n",
    "    return biomedical_datasets, space_biology_datasets\n",
    "\n",
    "def download_and_preprocess_data():\n",
    "    \"\"\"Download and preprocess selected datasets.\"\"\"\n",
    "    \n",
    "    # For demonstration, we'll create a simulated dataset\n",
    "    # In a real scenario, you would download data from the sources listed above\n",
    "    \n",
    "    # Simulated pre-training data (biomedical text data)\n",
    "    pretraining_texts = [\n",
    "        \"Rodents exposed to microgravity exhibit muscle atrophy and bone density loss.\",\n",
    "        \"Arabidopsis thaliana shows altered gene expression in spaceflight conditions.\",\n",
    "        # Add more examples\n",
    "    ]\n",
    "    \n",
    "    pretraining_labels = [\n",
    "        {\"organism\": \"Rodents\", \"condition\": \"microgravity\", \"effect\": \"muscle atrophy, bone density loss\"},\n",
    "        {\"organism\": \"Arabidopsis thaliana\", \"condition\": \"spaceflight\", \"effect\": \"altered gene expression\"},\n",
    "        # Add more examples\n",
    "    ]\n",
    "    \n",
    "    # Simulated fine-tuning data (space biology specific)\n",
    "    finetuning_texts = [\n",
    "        \"Caenorhabditis elegans cultured on the ISS showed changes in metabolic pathways related to oxidative stress.\",\n",
    "        \"Drosophila melanogaster reared in microgravity exhibited developmental delays and reduced lifespan.\",\n",
    "        # Add more examples\n",
    "    ]\n",
    "    \n",
    "    finetuning_labels = [\n",
    "        {\"organism\": \"Caenorhabditis elegans\", \"condition\": \"ISS culture\", \"effect\": \"changes in metabolic pathways, oxidative stress\"},\n",
    "        {\"organism\": \"Drosophila melanogaster\", \"condition\": \"microgravity\", \"effect\": \"developmental delays, reduced lifespan\"},\n",
    "        # Add more examples\n",
    "    ]\n",
    "    \n",
    "    # Convert to pandas DataFrames\n",
    "    pretraining_df = pd.DataFrame({\n",
    "        \"text\": pretraining_texts,\n",
    "        \"labels\": [str(label) for label in pretraining_labels]  # Convert dict to string for storage\n",
    "    })\n",
    "    \n",
    "    finetuning_df = pd.DataFrame({\n",
    "        \"text\": finetuning_texts,\n",
    "        \"labels\": [str(label) for label in finetuning_labels]\n",
    "    })\n",
    "    \n",
    "    # Save to disk\n",
    "    pretraining_df.to_csv(\"pretraining_data.csv\", index=False)\n",
    "    finetuning_df.to_csv(\"finetuning_data.csv\", index=False)\n",
    "    \n",
    "    return pretraining_df, finetuning_df\n",
    "\n",
    "# 2. Dataset Classes\n",
    "\n",
    "class SpaceBiologyDataset(Dataset):\n",
    "    \"\"\"Dataset class for BERT model.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension added by tokenizer\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        \n",
    "        # For BERT classification - you would need to define your label mapping\n",
    "        # This is simplified for demonstration\n",
    "        if isinstance(label, dict):\n",
    "            # This would need to be adjusted based on your specific task\n",
    "            encoding[\"labels\"] = torch.tensor(1)  # Placeholder\n",
    "        else:\n",
    "            encoding[\"labels\"] = torch.tensor(label)\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "class SpaceBiologyT5Dataset(Dataset):\n",
    "    \"\"\"Dataset class for T5 model.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # For T5, we format the input as \"extract info: {text}\"\n",
    "        input_text = f\"extract info: {text}\"\n",
    "        \n",
    "        # Convert label dict to string format: \"organism: X, condition: Y, effect: Z\"\n",
    "        if isinstance(label, dict):\n",
    "            target_text = \", \".join([f\"{k}: {v}\" for k, v in label.items()])\n",
    "        else:\n",
    "            target_text = str(label)\n",
    "        \n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension added by tokenizer\n",
    "        input_ids = input_encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = input_encoding[\"attention_mask\"].squeeze(0)\n",
    "        target_ids = target_encoding[\"input_ids\"].squeeze(0)\n",
    "        \n",
    "        # Replace padding token id with -100 for labels\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": target_ids\n",
    "        }\n",
    "\n",
    "# 3. Model Training Functions\n",
    "\n",
    "def train_bert_model(train_dataset, val_dataset, model_name, output_dir):\n",
    "    \"\"\"Train a BERT-based model for space biology information extraction.\"\"\"\n",
    "    \n",
    "    # Load pretrained model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2  # Adjust based on your task\n",
    "    )\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the final model\n",
    "    trainer.save_model(output_dir)\n",
    "    \n",
    "    return model, trainer\n",
    "\n",
    "def train_t5_model(train_dataset, val_dataset, model_name, output_dir):\n",
    "    \"\"\"Train a T5-based model for space biology information extraction.\"\"\"\n",
    "    \n",
    "    # Load pretrained model\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the final model\n",
    "    trainer.save_model(output_dir)\n",
    "    \n",
    "    return model, trainer\n",
    "\n",
    "# 4. Inference Functions\n",
    "\n",
    "def extract_info_bert(text, model, tokenizer, max_length=512):\n",
    "    \"\"\"Extract information using BERT model.\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # This implementation depends on your specific task\n",
    "    # This is a placeholder for demonstration\n",
    "    probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "    prediction = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def extract_info_t5(text, model, tokenizer, max_length=512):\n",
    "    \"\"\"Extract information using T5 model.\"\"\"\n",
    "    \n",
    "    input_text = f\"extract info: {text}\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Parse the output into a structured format\n",
    "    info_dict = {}\n",
    "    for item in decoded_output.split(\", \"):\n",
    "        if \":\" in item:\n",
    "            key, value = item.split(\":\", 1)\n",
    "            info_dict[key.strip()] = value.strip()\n",
    "    \n",
    "    return info_dict\n",
    "\n",
    "# 5. Main Pipeline Function\n",
    "\n",
    "def run_space_biology_model_pipeline():\n",
    "    \"\"\"Run the complete pipeline for space biology model development.\"\"\"\n",
    "    \n",
    "    # Set up wandb for tracking experiments\n",
    "    wandb.init(project=\"space-biology-model-zoo\")\n",
    "    \n",
    "    # 1. Dataset Collection\n",
    "    print(\"Creating dataset catalog...\")\n",
    "    biomedical_datasets, space_biology_datasets = create_dataset_catalog()\n",
    "    \n",
    "    # 2. Data Preprocessing\n",
    "    print(\"Downloading and preprocessing data...\")\n",
    "    pretraining_df, finetuning_df = download_and_preprocess_data()\n",
    "    \n",
    "    # 3. Split data for fine-tuning\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        finetuning_df[\"text\"].tolist(),\n",
    "        finetuning_df[\"labels\"].tolist(),\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 4. Model and Tokenizer Setup\n",
    "    if MODEL_TYPE == \"bert\":\n",
    "        print(f\"Loading BERT model: {BERT_MODEL_NAME}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = SpaceBiologyDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
    "        val_dataset = SpaceBiologyDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training BERT model...\")\n",
    "        model, trainer = train_bert_model(\n",
    "            train_dataset, \n",
    "            val_dataset, \n",
    "            BERT_MODEL_NAME, \n",
    "            OUTPUT_DIR\n",
    "        )\n",
    "        \n",
    "        # Example inference\n",
    "        sample_text = \"Astronauts exposed to microgravity show bone density loss after extended spaceflight.\"\n",
    "        prediction = extract_info_bert(sample_text, model, tokenizer)\n",
    "        print(f\"Sample prediction (BERT): {prediction}\")\n",
    "        \n",
    "    elif MODEL_TYPE == \"t5\":\n",
    "        print(f\"Loading T5 model: {T5_MODEL_NAME}\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(T5_MODEL_NAME)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = SpaceBiologyT5Dataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
    "        val_dataset = SpaceBiologyT5Dataset(val_texts, val_labels, tokenizer, MAX_LENGTH)\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training T5 model...\")\n",
    "        model, trainer = train_t5_model(\n",
    "            train_dataset, \n",
    "            val_dataset, \n",
    "            T5_MODEL_NAME, \n",
    "            OUTPUT_DIR\n",
    "        )\n",
    "        \n",
    "        # Example inference\n",
    "        sample_text = \"Astronauts exposed to microgravity show bone density loss after extended spaceflight.\"\n",
    "        prediction = extract_info_t5(sample_text, model, tokenizer)\n",
    "        print(f\"Sample prediction (T5): {prediction}\")\n",
    "    \n",
    "    # 5. Evaluation\n",
    "    print(\"Evaluating model performance...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    wandb.log({\"eval_loss\": eval_results[\"eval_loss\"]})\n",
    "    \n",
    "    # 6. Save model and artifacts\n",
    "    print(f\"Saving model to {OUTPUT_DIR}\")\n",
    "    \n",
    "    # Create model card\n",
    "    model_card = f\"\"\"\n",
    "    # Space Biology Information Extraction Model\n",
    "    \n",
    "    ## Model Description\n",
    "    - Model Type: {MODEL_TYPE.upper()}\n",
    "    - Base Model: {BERT_MODEL_NAME if MODEL_TYPE == \"bert\" else T5_MODEL_NAME}\n",
    "    - Task: Extract information about organisms, conditions, and effects from space biology text\n",
    "    \n",
    "    ## Training Data\n",
    "    - Pretrained on biomedical literature\n",
    "    - Fine-tuned on space biology datasets\n",
    "    \n",
    "    ## Performance\n",
    "    - Evaluation Loss: {eval_results[\"eval_loss\"]:.4f}\n",
    "    \n",
    "    ## Usage\n",
    "    ```python\n",
    "    # Example usage code for inference\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"{OUTPUT_DIR}\")\n",
    "    model = AutoModel.from_pretrained(\"{OUTPUT_DIR}\")\n",
    "    \n",
    "    # Run inference\n",
    "    text = \"Your space biology text here\"\n",
    "    # Follow the extract_info_{MODEL_TYPE} function for inference\n",
    "    ```\n",
    "    \n",
    "    ## Limitations\n",
    "    - This model is specifically designed for space biology text analysis\n",
    "    - Performance may vary on general biological text\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(f\"{OUTPUT_DIR}/MODEL_CARD.md\", \"w\") as f:\n",
    "        f.write(model_card)\n",
    "    \n",
    "    print(\"Pipeline completed successfully!\")\n",
    "    return model, tokenizer, eval_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_space_biology_model_pipeline()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter:\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "API key must be 40 characters long, yours was 8",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 493\u001B[0m\n\u001B[1;32m    490\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model, tokenizer, eval_results\n\u001B[1;32m    492\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 493\u001B[0m     \u001B[43mrun_space_biology_model_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 380\u001B[0m, in \u001B[0;36mrun_space_biology_model_pipeline\u001B[0;34m()\u001B[0m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Run the complete pipeline for space biology model development.\"\"\"\u001B[39;00m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;66;03m# Set up wandb for tracking experiments\u001B[39;00m\n\u001B[0;32m--> 380\u001B[0m \u001B[43mwandb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproject\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspace-biology-model-zoo\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[38;5;66;03m# 1. Dataset Collection\u001B[39;00m\n\u001B[1;32m    383\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating dataset catalog...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1482\u001B[0m, in \u001B[0;36minit\u001B[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001B[0m\n\u001B[1;32m   1478\u001B[0m     wl\u001B[38;5;241m.\u001B[39m_get_logger()\u001B[38;5;241m.\u001B[39mexception(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror in wandb.init()\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39me)\n\u001B[1;32m   1480\u001B[0m \u001B[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001B[39;00m\n\u001B[1;32m   1481\u001B[0m \u001B[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001B[39;00m\n\u001B[0;32m-> 1482\u001B[0m \u001B[43mwandb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sentry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1483\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/wandb/analytics/sentry.py:156\u001B[0m, in \u001B[0;36mSentry.reraise\u001B[0;34m(self, exc)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexception(exc)\n\u001B[1;32m    154\u001B[0m \u001B[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001B[39;00m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;66;03m# but hopefully it's not too bad\u001B[39;00m\n\u001B[0;32m--> 156\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\u001B[38;5;241m.\u001B[39mwith_traceback(sys\u001B[38;5;241m.\u001B[39mexc_info()[\u001B[38;5;241m2\u001B[39m])\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1427\u001B[0m, in \u001B[0;36minit\u001B[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001B[0m\n\u001B[1;32m   1423\u001B[0m wl \u001B[38;5;241m=\u001B[39m wandb_setup\u001B[38;5;241m.\u001B[39m_setup(start_service\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1425\u001B[0m wi \u001B[38;5;241m=\u001B[39m _WandbInit(wl, init_telemetry)\n\u001B[0;32m-> 1427\u001B[0m \u001B[43mwi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaybe_login\u001B[49m\u001B[43m(\u001B[49m\u001B[43minit_settings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1428\u001B[0m run_settings \u001B[38;5;241m=\u001B[39m wi\u001B[38;5;241m.\u001B[39mmake_run_settings(init_settings)\n\u001B[1;32m   1430\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_settings\u001B[38;5;241m.\u001B[39mrun_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:175\u001B[0m, in \u001B[0;36m_WandbInit.maybe_login\u001B[0;34m(self, init_settings)\u001B[0m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_settings\u001B[38;5;241m.\u001B[39m_noop \u001B[38;5;129;01mor\u001B[39;00m run_settings\u001B[38;5;241m.\u001B[39m_offline:\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 175\u001B[0m \u001B[43mwandb_login\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_login\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[43manonymous\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_settings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manonymous\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_settings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforce\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_disable_warning\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_silent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_settings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquiet\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_settings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_login.py:311\u001B[0m, in \u001B[0;36m_login\u001B[0;34m(anonymous, key, relogin, host, force, timeout, verify, _silent, _disable_warning)\u001B[0m\n\u001B[1;32m    308\u001B[0m     wlogin\u001B[38;5;241m.\u001B[39m_verify_login(key)\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m key_is_pre_configured:\n\u001B[0;32m--> 311\u001B[0m     \u001B[43mwlogin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtry_save_api_key\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    312\u001B[0m     wlogin\u001B[38;5;241m.\u001B[39mupdate_session(key, status\u001B[38;5;241m=\u001B[39mkey_status)\n\u001B[1;32m    313\u001B[0m     wlogin\u001B[38;5;241m.\u001B[39m_update_global_anonymous_setting()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_login.py:176\u001B[0m, in \u001B[0;36m_WandbLogin.try_save_api_key\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key:\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m         \u001B[43mapikey\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_key\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_settings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m apikey\u001B[38;5;241m.\u001B[39mWriteNetrcError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    178\u001B[0m         wandb\u001B[38;5;241m.\u001B[39mtermwarn(\u001B[38;5;28mstr\u001B[39m(e))\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/wandb/sdk/lib/apikey.py:294\u001B[0m, in \u001B[0;36mwrite_key\u001B[0;34m(settings, key, api)\u001B[0m\n\u001B[1;32m    291\u001B[0m _, suffix \u001B[38;5;241m=\u001B[39m key\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m key \u001B[38;5;28;01melse\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m, key)\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(suffix) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m40\u001B[39m:\n\u001B[0;32m--> 294\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    295\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAPI key must be 40 characters long, yours was \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mlen\u001B[39m(key))\n\u001B[1;32m    296\u001B[0m     )\n\u001B[1;32m    298\u001B[0m write_netrc(settings\u001B[38;5;241m.\u001B[39mbase_url, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, key)\n",
      "\u001B[0;31mValueError\u001B[0m: API key must be 40 characters long, yours was 8"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T07:05:38.893696Z",
     "start_time": "2025-03-29T07:05:38.737518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50V2\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, TimeDistributed, Attention, Input, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 224\n",
    "MAX_TEXT_LENGTH = 100\n",
    "VOCAB_SIZE = 5000\n",
    "EMBEDDING_DIM = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_space_biology_dataset(csv_path, img_dir):\n",
    "    \"\"\"\n",
    "    Load space biology image dataset with corresponding text descriptions\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to CSV file containing image filenames and descriptions\n",
    "        img_dir: Directory containing the images\n",
    "        \n",
    "    Returns:\n",
    "        images: List of preprocessed images\n",
    "        descriptions: List of text descriptions\n",
    "    \"\"\"\n",
    "    # Load metadata\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Image preprocessing with data augmentation for training\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=0.4  # 40% will be used for validation+testing\n",
    "    )\n",
    "    \n",
    "    # Generate batches of augmented image data\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        img_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=None,\n",
    "        subset='training'\n",
    "    )\n",
    "    \n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        img_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=None,\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    # Tokenize text descriptions\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(df['description'])\n",
    "    \n",
    "    # Convert text to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(df['description'])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=MAX_TEXT_LENGTH, padding='post')\n",
    "    \n",
    "    return train_generator, validation_generator, padded_sequences, tokenizer\n",
    "\n",
    "# Model 1: CNN-LSTM (EfficientNet + LSTM)\n",
    "def build_cnn_lstm_model():\n",
    "    \"\"\"\n",
    "    Build an image captioning model using EfficientNetB0 and LSTM\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Image feature extraction with EfficientNetB0\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Image input\n",
    "    image_input = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    encoded_image = base_model(image_input)\n",
    "    encoded_image = Flatten()(encoded_image)\n",
    "    encoded_image = Dense(EMBEDDING_DIM, activation='relu')(encoded_image)\n",
    "    \n",
    "    # Text input\n",
    "    text_input = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "    embedding = Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)(text_input)\n",
    "    \n",
    "    # Decode combined features\n",
    "    decoder = LSTM(512, return_sequences=True)(embedding)\n",
    "    decoder = LSTM(512)(decoder)\n",
    "    \n",
    "    # Combine image and text features\n",
    "    decoder_combined = tf.concat([encoded_image, decoder], axis=-1)\n",
    "    output = Dense(512, activation='relu')(decoder_combined)\n",
    "    output = Dropout(0.3)(output)\n",
    "    output = Dense(VOCAB_SIZE, activation='softmax')(output)\n",
    "    \n",
    "    model = Model(inputs=[image_input, text_input], outputs=output)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Model 2: CNN-Transformer (ResNet + Transformer)\n",
    "def build_cnn_transformer_model():\n",
    "    \"\"\"\n",
    "    Build an image captioning model using ResNet50V2 and Transformer\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Image feature extraction with ResNet50V2\n",
    "    base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Image input\n",
    "    image_input = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    encoded_image = base_model(image_input)\n",
    "    encoded_image = Flatten()(encoded_image)\n",
    "    encoded_image = Dense(EMBEDDING_DIM, activation='relu')(encoded_image)\n",
    "    \n",
    "    # Text input\n",
    "    text_input = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "    embedding = Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)(text_input)\n",
    "    \n",
    "    # Transformer layers (simplified)\n",
    "    # Note: For a full transformer implementation, you'd use multiple self-attention layers\n",
    "    attention_output = Attention()([embedding, embedding])\n",
    "    decoder = TimeDistributed(Dense(512, activation='relu'))(attention_output)\n",
    "    decoder = Flatten()(decoder)\n",
    "    \n",
    "    # Combine image and text features\n",
    "    decoder_combined = tf.concat([encoded_image, decoder], axis=-1)\n",
    "    output = Dense(512, activation='relu')(decoder_combined)\n",
    "    output = Dropout(0.3)(output)\n",
    "    output = Dense(VOCAB_SIZE, activation='softmax')(output)\n",
    "    \n",
    "    model = Model(inputs=[image_input, text_input], outputs=output)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Training function\n",
    "def train_and_evaluate_models(train_gen, val_gen, text_data, tokenizer):\n",
    "    \"\"\"\n",
    "    Train and evaluate both models\n",
    "    \n",
    "    Args:\n",
    "        train_gen: Training data generator\n",
    "        val_gen: Validation data generator\n",
    "        text_data: Processed text data\n",
    "        tokenizer: Text tokenizer\n",
    "        \n",
    "    Returns:\n",
    "        models: Dictionary containing trained models\n",
    "        histories: Dictionary containing training histories\n",
    "    \"\"\"\n",
    "    # Initialize models\n",
    "    cnn_lstm_model = build_cnn_lstm_model()\n",
    "    cnn_transformer_model = build_cnn_transformer_model()\n",
    "    \n",
    "    # Split validation data into validation and test sets\n",
    "    val_images = []\n",
    "    for i in range(len(val_gen)):\n",
    "        batch = val_gen[i]\n",
    "        val_images.extend(batch)\n",
    "        if len(val_images) >= 40:  # 40 = 20 validation + 20 test samples\n",
    "            break\n",
    "    \n",
    "    val_images = np.array(val_images[:40])\n",
    "    val_text = text_data[-40:]\n",
    "    \n",
    "    val_images_split, test_images, val_text_split, test_text = train_test_split(\n",
    "        val_images, val_text, test_size=0.5, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train CNN-LSTM model\n",
    "    cnn_lstm_history = cnn_lstm_model.fit(\n",
    "        [train_gen, np.ones((len(train_gen), MAX_TEXT_LENGTH))],  # Placeholder for text input during training\n",
    "        np.ones((len(train_gen), VOCAB_SIZE)),  # Placeholder for target\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(\n",
    "            [val_images_split, np.ones((len(val_images_split), MAX_TEXT_LENGTH))],\n",
    "            np.ones((len(val_images_split), VOCAB_SIZE))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Train CNN-Transformer model\n",
    "    cnn_transformer_history = cnn_transformer_model.fit(\n",
    "        [train_gen, np.ones((len(train_gen), MAX_TEXT_LENGTH))],  # Placeholder for text input during training\n",
    "        np.ones((len(train_gen), VOCAB_SIZE)),  # Placeholder for target\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(\n",
    "            [val_images_split, np.ones((len(val_images_split), MAX_TEXT_LENGTH))],\n",
    "            np.ones((len(val_images_split), VOCAB_SIZE))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    cnn_lstm_results = cnn_lstm_model.evaluate(\n",
    "        [test_images, np.ones((len(test_images), MAX_TEXT_LENGTH))],\n",
    "        np.ones((len(test_images), VOCAB_SIZE))\n",
    "    )\n",
    "    \n",
    "    cnn_transformer_results = cnn_transformer_model.evaluate(\n",
    "        [test_images, np.ones((len(test_images), MAX_TEXT_LENGTH))],\n",
    "        np.ones((len(test_images), VOCAB_SIZE))\n",
    "    )\n",
    "    \n",
    "    print(\"CNN-LSTM Test Results:\", cnn_lstm_results)\n",
    "    print(\"CNN-Transformer Test Results:\", cnn_transformer_results)\n",
    "    \n",
    "    models = {\n",
    "        \"cnn_lstm\": cnn_lstm_model,\n",
    "        \"cnn_transformer\": cnn_transformer_model\n",
    "    }\n",
    "    \n",
    "    histories = {\n",
    "        \"cnn_lstm\": cnn_lstm_history,\n",
    "        \"cnn_transformer\": cnn_transformer_history\n",
    "    }\n",
    "    \n",
    "    return models, histories\n",
    "\n",
    "# Generate captions from images\n",
    "def generate_caption(model, image, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate a caption for a given image\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        image: Input image\n",
    "        tokenizer: Text tokenizer\n",
    "        \n",
    "    Returns:\n",
    "        caption: Generated caption\n",
    "    \"\"\"\n",
    "    # Preprocess image\n",
    "    processed_image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    processed_image = processed_image / 255.0\n",
    "    processed_image = tf.expand_dims(processed_image, 0)\n",
    "    \n",
    "    # Start with a blank sequence\n",
    "    caption = ['<start>']\n",
    "    \n",
    "    # Generate the caption word by word\n",
    "    for i in range(MAX_TEXT_LENGTH):\n",
    "        sequence = tokenizer.texts_to_sequences([' '.join(caption)])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=MAX_TEXT_LENGTH, padding='post')\n",
    "        \n",
    "        # Predict next word\n",
    "        prediction = model.predict([processed_image, sequence], verbose=0)\n",
    "        predicted_id = np.argmax(prediction)\n",
    "        \n",
    "        # Convert id to word\n",
    "        word = tokenizer.index_word.get(predicted_id, '')\n",
    "        \n",
    "        # Stop if we predict the end token\n",
    "        if word == '<end>' or word == '':\n",
    "            break\n",
    "            \n",
    "        # Add predicted word to caption\n",
    "        caption.append(word)\n",
    "    \n",
    "    # Remove start token\n",
    "    return ' '.join(caption[1:])\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Path to dataset (example paths)\n",
    "    csv_path = 'space_biology_dataset.csv'\n",
    "    img_dir = 'space_biology_images'\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    train_gen, val_gen, text_data, tokenizer = load_space_biology_dataset(csv_path, img_dir)\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    models, histories = train_and_evaluate_models(train_gen, val_gen, text_data, tokenizer)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(histories['cnn_lstm'].history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(histories['cnn_lstm'].history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('CNN-LSTM Model')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(histories['cnn_transformer'].history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(histories['cnn_transformer'].history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('CNN-Transformer Model')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    \n",
    "    # Save models\n",
    "    models['cnn_lstm'].save('cnn_lstm_space_biology.h5')\n",
    "    models['cnn_transformer'].save('cnn_transformer_space_biology.h5')\n",
    "    \n",
    "    # Save tokenizer\n",
    "    import pickle\n",
    "    with open('tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print(\"Models and tokenizer saved successfully!\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "bf40d05248b917ea",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'space_biology_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 346\u001B[0m\n\u001B[1;32m    343\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModels and tokenizer saved successfully!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    345\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 346\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 307\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    304\u001B[0m img_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspace_biology_images\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    306\u001B[0m \u001B[38;5;66;03m# Load and preprocess data\u001B[39;00m\n\u001B[0;32m--> 307\u001B[0m train_gen, val_gen, text_data, tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mload_space_biology_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcsv_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;66;03m# Train and evaluate models\u001B[39;00m\n\u001B[1;32m    310\u001B[0m models, histories \u001B[38;5;241m=\u001B[39m train_and_evaluate_models(train_gen, val_gen, text_data, tokenizer)\n",
      "Cell \u001B[0;32mIn[5], line 39\u001B[0m, in \u001B[0;36mload_space_biology_dataset\u001B[0;34m(csv_path, img_dir)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;124;03mLoad space biology image dataset with corresponding text descriptions\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;124;03m\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;124;03m    descriptions: List of text descriptions\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# Load metadata\u001B[39;00m\n\u001B[0;32m---> 39\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcsv_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Image preprocessing with data augmentation for training\u001B[39;00m\n\u001B[1;32m     42\u001B[0m datagen \u001B[38;5;241m=\u001B[39m ImageDataGenerator(\n\u001B[1;32m     43\u001B[0m     rescale\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m255\u001B[39m,\n\u001B[1;32m     44\u001B[0m     rotation_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     51\u001B[0m     validation_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.4\u001B[39m  \u001B[38;5;66;03m# 40% will be used for validation+testing\u001B[39;00m\n\u001B[1;32m     52\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'space_biology_dataset.csv'"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
