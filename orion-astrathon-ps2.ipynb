{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    T5ForConditionalGeneration, \n",
    "    T5Tokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "MODEL_TYPE = \"bert\"  # or \"t5\"\n",
    "BERT_MODEL_NAME = \"dmis-lab/biobert-v1.1\"  # or \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "T5_MODEL_NAME = \"razent/SciFive-base-Pubmed_PMC\"  # or \"t5-small\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "OUTPUT_DIR = \"./space_biology_model\"\n",
    "\n",
    "# 1. Dataset Collection and Preprocessing\n",
    "\n",
    "def create_dataset_catalog():\n",
    "    \"\"\"Create a catalog of available biomedical and space biology datasets.\"\"\"\n",
    "    \n",
    "    biomedical_datasets = [\n",
    "        {\n",
    "            \"name\": \"PubMed Abstracts (Space Biology)\",\n",
    "            \"source\": \"PubMed\",\n",
    "            \"data_type\": \"text\",\n",
    "            \"size\": \"~10,000 abstracts\",\n",
    "            \"url\": \"https://pubmed.ncbi.nlm.nih.gov/\",\n",
    "            \"description\": \"Scientific abstracts related to space biology research\",\n",
    "            \"domains\": [\"microgravity\", \"radiation\", \"space adaptation\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"UniProt Space Biology Entries\",\n",
    "            \"source\": \"UniProt\",\n",
    "            \"data_type\": \"protein annotations\",\n",
    "            \"size\": \"~1,000 entries\",\n",
    "            \"url\": \"https://www.uniprot.org/\",\n",
    "            \"description\": \"Protein annotations for organisms studied in space\",\n",
    "            \"domains\": [\"protein function\", \"structural biology\"]\n",
    "        }\n",
    "        # Add more datasets as needed\n",
    "    ]\n",
    "    \n",
    "    space_biology_datasets = [\n",
    "        {\n",
    "            \"name\": \"NASA GeneLab\",\n",
    "            \"source\": \"NASA\",\n",
    "            \"data_type\": \"omics data + metadata\",\n",
    "            \"size\": \"500+ datasets\",\n",
    "            \"url\": \"https://genelab.nasa.gov/\",\n",
    "            \"description\": \"Omics data from space biology experiments\",\n",
    "            \"domains\": [\"transcriptomics\", \"proteomics\", \"genomics\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"OSDR WGS Dataset 466\",\n",
    "            \"source\": \"NASA OSDR\",\n",
    "            \"data_type\": \"genomic + metadata\",\n",
    "            \"size\": \"1 dataset\",\n",
    "            \"url\": \"https://osdr.nasa.gov/bio/repo/data/studies/OSD-466\",\n",
    "            \"description\": \"Whole Genome Sequencing data with descriptions\",\n",
    "            \"domains\": [\"genomics\", \"microbiology\"]\n",
    "        }\n",
    "        # Add more datasets as needed\n",
    "    ]\n",
    "    \n",
    "    # Save to CSV files\n",
    "    pd.DataFrame(biomedical_datasets).to_csv(\"biomedical_datasets.csv\", index=False)\n",
    "    pd.DataFrame(space_biology_datasets).to_csv(\"space_biology_datasets.csv\", index=False)\n",
    "    \n",
    "    return biomedical_datasets, space_biology_datasets\n",
    "\n",
    "def download_and_preprocess_data():\n",
    "    \"\"\"Download and preprocess selected datasets.\"\"\"\n",
    "    \n",
    "    # For demonstration, we'll create a simulated dataset\n",
    "    # In a real scenario, you would download data from the sources listed above\n",
    "    \n",
    "    # Simulated pre-training data (biomedical text data)\n",
    "    pretraining_texts = [\n",
    "        \"Rodents exposed to microgravity exhibit muscle atrophy and bone density loss.\",\n",
    "        \"Arabidopsis thaliana shows altered gene expression in spaceflight conditions.\",\n",
    "        # Add more examples\n",
    "    ]\n",
    "    \n",
    "    pretraining_labels = [\n",
    "        {\"organism\": \"Rodents\", \"condition\": \"microgravity\", \"effect\": \"muscle atrophy, bone density loss\"},\n",
    "        {\"organism\": \"Arabidopsis thaliana\", \"condition\": \"spaceflight\", \"effect\": \"altered gene expression\"},\n",
    "        # Add more examples\n",
    "    ]\n",
    "    \n",
    "    # Simulated fine-tuning data (space biology specific)\n",
    "    finetuning_texts = [\n",
    "        \"Caenorhabditis elegans cultured on the ISS showed changes in metabolic pathways related to oxidative stress.\",\n",
    "        \"Drosophila melanogaster reared in microgravity exhibited developmental delays and reduced lifespan.\",\n",
    "        # Add more examples\n",
    "    ]\n",
    "    \n",
    "    finetuning_labels = [\n",
    "        {\"organism\": \"Caenorhabditis elegans\", \"condition\": \"ISS culture\", \"effect\": \"changes in metabolic pathways, oxidative stress\"},\n",
    "        {\"organism\": \"Drosophila melanogaster\", \"condition\": \"microgravity\", \"effect\": \"developmental delays, reduced lifespan\"},\n",
    "        # Add more examples\n",
    "    ]\n",
    "    \n",
    "    # Convert to pandas DataFrames\n",
    "    pretraining_df = pd.DataFrame({\n",
    "        \"text\": pretraining_texts,\n",
    "        \"labels\": [str(label) for label in pretraining_labels]  # Convert dict to string for storage\n",
    "    })\n",
    "    \n",
    "    finetuning_df = pd.DataFrame({\n",
    "        \"text\": finetuning_texts,\n",
    "        \"labels\": [str(label) for label in finetuning_labels]\n",
    "    })\n",
    "    \n",
    "    # Save to disk\n",
    "    pretraining_df.to_csv(\"pretraining_data.csv\", index=False)\n",
    "    finetuning_df.to_csv(\"finetuning_data.csv\", index=False)\n",
    "    \n",
    "    return pretraining_df, finetuning_df\n",
    "\n",
    "# 2. Dataset Classes\n",
    "\n",
    "class SpaceBiologyDataset(Dataset):\n",
    "    \"\"\"Dataset class for BERT model.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension added by tokenizer\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        \n",
    "        # For BERT classification - you would need to define your label mapping\n",
    "        # This is simplified for demonstration\n",
    "        if isinstance(label, dict):\n",
    "            # This would need to be adjusted based on your specific task\n",
    "            encoding[\"labels\"] = torch.tensor(1)  # Placeholder\n",
    "        else:\n",
    "            encoding[\"labels\"] = torch.tensor(label)\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "class SpaceBiologyT5Dataset(Dataset):\n",
    "    \"\"\"Dataset class for T5 model.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # For T5, we format the input as \"extract info: {text}\"\n",
    "        input_text = f\"extract info: {text}\"\n",
    "        \n",
    "        # Convert label dict to string format: \"organism: X, condition: Y, effect: Z\"\n",
    "        if isinstance(label, dict):\n",
    "            target_text = \", \".join([f\"{k}: {v}\" for k, v in label.items()])\n",
    "        else:\n",
    "            target_text = str(label)\n",
    "        \n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension added by tokenizer\n",
    "        input_ids = input_encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = input_encoding[\"attention_mask\"].squeeze(0)\n",
    "        target_ids = target_encoding[\"input_ids\"].squeeze(0)\n",
    "        \n",
    "        # Replace padding token id with -100 for labels\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": target_ids\n",
    "        }\n",
    "\n",
    "# 3. Model Training Functions\n",
    "\n",
    "def train_bert_model(train_dataset, val_dataset, model_name, output_dir):\n",
    "    \"\"\"Train a BERT-based model for space biology information extraction.\"\"\"\n",
    "    \n",
    "    # Load pretrained model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2  # Adjust based on your task\n",
    "    )\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the final model\n",
    "    trainer.save_model(output_dir)\n",
    "    \n",
    "    return model, trainer\n",
    "\n",
    "def train_t5_model(train_dataset, val_dataset, model_name, output_dir):\n",
    "    \"\"\"Train a T5-based model for space biology information extraction.\"\"\"\n",
    "    \n",
    "    # Load pretrained model\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the final model\n",
    "    trainer.save_model(output_dir)\n",
    "    \n",
    "    return model, trainer\n",
    "\n",
    "# 4. Inference Functions\n",
    "\n",
    "def extract_info_bert(text, model, tokenizer, max_length=512):\n",
    "    \"\"\"Extract information using BERT model.\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # This implementation depends on your specific task\n",
    "    # This is a placeholder for demonstration\n",
    "    probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "    prediction = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def extract_info_t5(text, model, tokenizer, max_length=512):\n",
    "    \"\"\"Extract information using T5 model.\"\"\"\n",
    "    \n",
    "    input_text = f\"extract info: {text}\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Parse the output into a structured format\n",
    "    info_dict = {}\n",
    "    for item in decoded_output.split(\", \"):\n",
    "        if \":\" in item:\n",
    "            key, value = item.split(\":\", 1)\n",
    "            info_dict[key.strip()] = value.strip()\n",
    "    \n",
    "    return info_dict\n",
    "\n",
    "# 5. Main Pipeline Function\n",
    "\n",
    "def run_space_biology_model_pipeline():\n",
    "    \"\"\"Run the complete pipeline for space biology model development.\"\"\"\n",
    "    \n",
    "    # Set up wandb for tracking experiments\n",
    "    wandb.init(project=\"space-biology-model-zoo\")\n",
    "    \n",
    "    # 1. Dataset Collection\n",
    "    print(\"Creating dataset catalog...\")\n",
    "    biomedical_datasets, space_biology_datasets = create_dataset_catalog()\n",
    "    \n",
    "    # 2. Data Preprocessing\n",
    "    print(\"Downloading and preprocessing data...\")\n",
    "    pretraining_df, finetuning_df = download_and_preprocess_data()\n",
    "    \n",
    "    # 3. Split data for fine-tuning\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        finetuning_df[\"text\"].tolist(),\n",
    "        finetuning_df[\"labels\"].tolist(),\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 4. Model and Tokenizer Setup\n",
    "    if MODEL_TYPE == \"bert\":\n",
    "        print(f\"Loading BERT model: {BERT_MODEL_NAME}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = SpaceBiologyDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
    "        val_dataset = SpaceBiologyDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training BERT model...\")\n",
    "        model, trainer = train_bert_model(\n",
    "            train_dataset, \n",
    "            val_dataset, \n",
    "            BERT_MODEL_NAME, \n",
    "            OUTPUT_DIR\n",
    "        )\n",
    "        \n",
    "        # Example inference\n",
    "        sample_text = \"Astronauts exposed to microgravity show bone density loss after extended spaceflight.\"\n",
    "        prediction = extract_info_bert(sample_text, model, tokenizer)\n",
    "        print(f\"Sample prediction (BERT): {prediction}\")\n",
    "        \n",
    "    elif MODEL_TYPE == \"t5\":\n",
    "        print(f\"Loading T5 model: {T5_MODEL_NAME}\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(T5_MODEL_NAME)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = SpaceBiologyT5Dataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
    "        val_dataset = SpaceBiologyT5Dataset(val_texts, val_labels, tokenizer, MAX_LENGTH)\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training T5 model...\")\n",
    "        model, trainer = train_t5_model(\n",
    "            train_dataset, \n",
    "            val_dataset, \n",
    "            T5_MODEL_NAME, \n",
    "            OUTPUT_DIR\n",
    "        )\n",
    "        \n",
    "        # Example inference\n",
    "        sample_text = \"Astronauts exposed to microgravity show bone density loss after extended spaceflight.\"\n",
    "        prediction = extract_info_t5(sample_text, model, tokenizer)\n",
    "        print(f\"Sample prediction (T5): {prediction}\")\n",
    "    \n",
    "    # 5. Evaluation\n",
    "    print(\"Evaluating model performance...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    wandb.log({\"eval_loss\": eval_results[\"eval_loss\"]})\n",
    "    \n",
    "    # 6. Save model and artifacts\n",
    "    print(f\"Saving model to {OUTPUT_DIR}\")\n",
    "    \n",
    "    # Create model card\n",
    "    model_card = f\"\"\"\n",
    "    # Space Biology Information Extraction Model\n",
    "    \n",
    "    ## Model Description\n",
    "    - Model Type: {MODEL_TYPE.upper()}\n",
    "    - Base Model: {BERT_MODEL_NAME if MODEL_TYPE == \"bert\" else T5_MODEL_NAME}\n",
    "    - Task: Extract information about organisms, conditions, and effects from space biology text\n",
    "    \n",
    "    ## Training Data\n",
    "    - Pretrained on biomedical literature\n",
    "    - Fine-tuned on space biology datasets\n",
    "    \n",
    "    ## Performance\n",
    "    - Evaluation Loss: {eval_results[\"eval_loss\"]:.4f}\n",
    "    \n",
    "    ## Usage\n",
    "    ```python\n",
    "    # Example usage code for inference\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"{OUTPUT_DIR}\")\n",
    "    model = AutoModel.from_pretrained(\"{OUTPUT_DIR}\")\n",
    "    \n",
    "    # Run inference\n",
    "    text = \"Your space biology text here\"\n",
    "    # Follow the extract_info_{MODEL_TYPE} function for inference\n",
    "    ```\n",
    "    \n",
    "    ## Limitations\n",
    "    - This model is specifically designed for space biology text analysis\n",
    "    - Performance may vary on general biological text\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(f\"{OUTPUT_DIR}/MODEL_CARD.md\", \"w\") as f:\n",
    "        f.write(model_card)\n",
    "    \n",
    "    print(\"Pipeline completed successfully!\")\n",
    "    return model, tokenizer, eval_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_space_biology_model_pipeline()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50V2\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, TimeDistributed, Attention, Input, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 224\n",
    "MAX_TEXT_LENGTH = 100\n",
    "VOCAB_SIZE = 5000\n",
    "EMBEDDING_DIM = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_space_biology_dataset(csv_path, img_dir):\n",
    "    \"\"\"\n",
    "    Load space biology image dataset with corresponding text descriptions\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to CSV file containing image filenames and descriptions\n",
    "        img_dir: Directory containing the images\n",
    "        \n",
    "    Returns:\n",
    "        images: List of preprocessed images\n",
    "        descriptions: List of text descriptions\n",
    "    \"\"\"\n",
    "    # Load metadata\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Image preprocessing with data augmentation for training\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=0.4  # 40% will be used for validation+testing\n",
    "    )\n",
    "    \n",
    "    # Generate batches of augmented image data\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        img_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=None,\n",
    "        subset='training'\n",
    "    )\n",
    "    \n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        img_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=None,\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    # Tokenize text descriptions\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(df['description'])\n",
    "    \n",
    "    # Convert text to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(df['description'])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=MAX_TEXT_LENGTH, padding='post')\n",
    "    \n",
    "    return train_generator, validation_generator, padded_sequences, tokenizer\n",
    "\n",
    "# Model 1: CNN-LSTM (EfficientNet + LSTM)\n",
    "def build_cnn_lstm_model():\n",
    "    \"\"\"\n",
    "    Build an image captioning model using EfficientNetB0 and LSTM\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Image feature extraction with EfficientNetB0\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Image input\n",
    "    image_input = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    encoded_image = base_model(image_input)\n",
    "    encoded_image = Flatten()(encoded_image)\n",
    "    encoded_image = Dense(EMBEDDING_DIM, activation='relu')(encoded_image)\n",
    "    \n",
    "    # Text input\n",
    "    text_input = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "    embedding = Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)(text_input)\n",
    "    \n",
    "    # Decode combined features\n",
    "    decoder = LSTM(512, return_sequences=True)(embedding)\n",
    "    decoder = LSTM(512)(decoder)\n",
    "    \n",
    "    # Combine image and text features\n",
    "    decoder_combined = tf.concat([encoded_image, decoder], axis=-1)\n",
    "    output = Dense(512, activation='relu')(decoder_combined)\n",
    "    output = Dropout(0.3)(output)\n",
    "    output = Dense(VOCAB_SIZE, activation='softmax')(output)\n",
    "    \n",
    "    model = Model(inputs=[image_input, text_input], outputs=output)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Model 2: CNN-Transformer (ResNet + Transformer)\n",
    "def build_cnn_transformer_model():\n",
    "    \"\"\"\n",
    "    Build an image captioning model using ResNet50V2 and Transformer\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Image feature extraction with ResNet50V2\n",
    "    base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Image input\n",
    "    image_input = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    encoded_image = base_model(image_input)\n",
    "    encoded_image = Flatten()(encoded_image)\n",
    "    encoded_image = Dense(EMBEDDING_DIM, activation='relu')(encoded_image)\n",
    "    \n",
    "    # Text input\n",
    "    text_input = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "    embedding = Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True)(text_input)\n",
    "    \n",
    "    # Transformer layers (simplified)\n",
    "    # Note: For a full transformer implementation, you'd use multiple self-attention layers\n",
    "    attention_output = Attention()([embedding, embedding])\n",
    "    decoder = TimeDistributed(Dense(512, activation='relu'))(attention_output)\n",
    "    decoder = Flatten()(decoder)\n",
    "    \n",
    "    # Combine image and text features\n",
    "    decoder_combined = tf.concat([encoded_image, decoder], axis=-1)\n",
    "    output = Dense(512, activation='relu')(decoder_combined)\n",
    "    output = Dropout(0.3)(output)\n",
    "    output = Dense(VOCAB_SIZE, activation='softmax')(output)\n",
    "    \n",
    "    model = Model(inputs=[image_input, text_input], outputs=output)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Training function\n",
    "def train_and_evaluate_models(train_gen, val_gen, text_data, tokenizer):\n",
    "    \"\"\"\n",
    "    Train and evaluate both models\n",
    "    \n",
    "    Args:\n",
    "        train_gen: Training data generator\n",
    "        val_gen: Validation data generator\n",
    "        text_data: Processed text data\n",
    "        tokenizer: Text tokenizer\n",
    "        \n",
    "    Returns:\n",
    "        models: Dictionary containing trained models\n",
    "        histories: Dictionary containing training histories\n",
    "    \"\"\"\n",
    "    # Initialize models\n",
    "    cnn_lstm_model = build_cnn_lstm_model()\n",
    "    cnn_transformer_model = build_cnn_transformer_model()\n",
    "    \n",
    "    # Split validation data into validation and test sets\n",
    "    val_images = []\n",
    "    for i in range(len(val_gen)):\n",
    "        batch = val_gen[i]\n",
    "        val_images.extend(batch)\n",
    "        if len(val_images) >= 40:  # 40 = 20 validation + 20 test samples\n",
    "            break\n",
    "    \n",
    "    val_images = np.array(val_images[:40])\n",
    "    val_text = text_data[-40:]\n",
    "    \n",
    "    val_images_split, test_images, val_text_split, test_text = train_test_split(\n",
    "        val_images, val_text, test_size=0.5, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train CNN-LSTM model\n",
    "    cnn_lstm_history = cnn_lstm_model.fit(\n",
    "        [train_gen, np.ones((len(train_gen), MAX_TEXT_LENGTH))],  # Placeholder for text input during training\n",
    "        np.ones((len(train_gen), VOCAB_SIZE)),  # Placeholder for target\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(\n",
    "            [val_images_split, np.ones((len(val_images_split), MAX_TEXT_LENGTH))],\n",
    "            np.ones((len(val_images_split), VOCAB_SIZE))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Train CNN-Transformer model\n",
    "    cnn_transformer_history = cnn_transformer_model.fit(\n",
    "        [train_gen, np.ones((len(train_gen), MAX_TEXT_LENGTH))],  # Placeholder for text input during training\n",
    "        np.ones((len(train_gen), VOCAB_SIZE)),  # Placeholder for target\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(\n",
    "            [val_images_split, np.ones((len(val_images_split), MAX_TEXT_LENGTH))],\n",
    "            np.ones((len(val_images_split), VOCAB_SIZE))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    cnn_lstm_results = cnn_lstm_model.evaluate(\n",
    "        [test_images, np.ones((len(test_images), MAX_TEXT_LENGTH))],\n",
    "        np.ones((len(test_images), VOCAB_SIZE))\n",
    "    )\n",
    "    \n",
    "    cnn_transformer_results = cnn_transformer_model.evaluate(\n",
    "        [test_images, np.ones((len(test_images), MAX_TEXT_LENGTH))],\n",
    "        np.ones((len(test_images), VOCAB_SIZE))\n",
    "    )\n",
    "    \n",
    "    print(\"CNN-LSTM Test Results:\", cnn_lstm_results)\n",
    "    print(\"CNN-Transformer Test Results:\", cnn_transformer_results)\n",
    "    \n",
    "    models = {\n",
    "        \"cnn_lstm\": cnn_lstm_model,\n",
    "        \"cnn_transformer\": cnn_transformer_model\n",
    "    }\n",
    "    \n",
    "    histories = {\n",
    "        \"cnn_lstm\": cnn_lstm_history,\n",
    "        \"cnn_transformer\": cnn_transformer_history\n",
    "    }\n",
    "    \n",
    "    return models, histories\n",
    "\n",
    "# Generate captions from images\n",
    "def generate_caption(model, image, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate a caption for a given image\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        image: Input image\n",
    "        tokenizer: Text tokenizer\n",
    "        \n",
    "    Returns:\n",
    "        caption: Generated caption\n",
    "    \"\"\"\n",
    "    # Preprocess image\n",
    "    processed_image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    processed_image = processed_image / 255.0\n",
    "    processed_image = tf.expand_dims(processed_image, 0)\n",
    "    \n",
    "    # Start with a blank sequence\n",
    "    caption = ['<start>']\n",
    "    \n",
    "    # Generate the caption word by word\n",
    "    for i in range(MAX_TEXT_LENGTH):\n",
    "        sequence = tokenizer.texts_to_sequences([' '.join(caption)])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=MAX_TEXT_LENGTH, padding='post')\n",
    "        \n",
    "        # Predict next word\n",
    "        prediction = model.predict([processed_image, sequence], verbose=0)\n",
    "        predicted_id = np.argmax(prediction)\n",
    "        \n",
    "        # Convert id to word\n",
    "        word = tokenizer.index_word.get(predicted_id, '')\n",
    "        \n",
    "        # Stop if we predict the end token\n",
    "        if word == '<end>' or word == '':\n",
    "            break\n",
    "            \n",
    "        # Add predicted word to caption\n",
    "        caption.append(word)\n",
    "    \n",
    "    # Remove start token\n",
    "    return ' '.join(caption[1:])\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Path to dataset (example paths)\n",
    "    csv_path = 'space_biology_dataset.csv'\n",
    "    img_dir = 'space_biology_images'\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    train_gen, val_gen, text_data, tokenizer = load_space_biology_dataset(csv_path, img_dir)\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    models, histories = train_and_evaluate_models(train_gen, val_gen, text_data, tokenizer)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(histories['cnn_lstm'].history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(histories['cnn_lstm'].history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('CNN-LSTM Model')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(histories['cnn_transformer'].history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(histories['cnn_transformer'].history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('CNN-Transformer Model')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    \n",
    "    # Save models\n",
    "    models['cnn_lstm'].save('cnn_lstm_space_biology.h5')\n",
    "    models['cnn_transformer'].save('cnn_transformer_space_biology.h5')\n",
    "    \n",
    "    # Save tokenizer\n",
    "    import pickle\n",
    "    with open('tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print(\"Models and tokenizer saved successfully!\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "bf40d05248b917ea",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
